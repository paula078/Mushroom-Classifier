{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299953a-0ddd-4e88-8b77-e9d466052178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/cleanlab/cleanvision.git\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78ef28-45a7-4018-9b27-3e239ba9484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_folders(directory):\n",
    "    total_files, total_folders = 0, 0\n",
    "\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            num_files = sum(1 for name in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, name)))\n",
    "            print(f\"Folder '{item}' contains {num_files} file(s).\")\n",
    "            total_files += num_files\n",
    "            total_folders += 1\n",
    "\n",
    "    if total_folders > 0:\n",
    "        avg_num_files = total_files / total_folders\n",
    "        print(f\"\\nAverage number of files per folder is: {avg_num_files:.2f}\")\n",
    "    else:\n",
    "        print(\"No folders found.\")\n",
    "\n",
    "DIRECTORY = 'data/Mushrooms'\n",
    "count_files_in_folders(DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e3b95-a121-4eed-92c6-c3d2f9b4ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5a587-924e-453a-aca5-24c93d322c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(directory):\n",
    "    image_paths = []\n",
    "\n",
    "    for rel_path1 in os.listdir(directory):\n",
    "        subdir_path = os.path.join(directory, rel_path1)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for rel_path2 in tqdm(os.listdir(subdir_path)):\n",
    "                full_path = os.path.join(subdir_path, rel_path2)\n",
    "                if os.path.isfile(full_path):\n",
    "                    image_paths.append(full_path)\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "def split_train_test(image_paths, test_ratio=0.1):\n",
    "    random.shuffle(image_paths)\n",
    "    split_index = int(len(image_paths) * test_ratio)\n",
    "    test_image_paths = image_paths[:split_index]\n",
    "    train_image_paths = image_paths[split_index:]\n",
    "    return train_image_paths, test_image_paths\n",
    "\n",
    "image_paths = get_image_paths(DIRECTORY)\n",
    "train_image_paths, test_image_paths = split_train_test(image_paths)\n",
    "\n",
    "print(f\"Number of images:\\nTest: {len(test_image_paths)}\\nTrain: {len(train_image_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14569ea3-54f5-456f-8b13-a434f416cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_to_index_mapping(directory):\n",
    "    class_names = sorted(os.listdir(directory))\n",
    "    class_to_index = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    return class_to_index\n",
    "\n",
    "class_to_index = create_class_to_index_mapping(DIRECTORY)\n",
    "print(class_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c24e27-3647-4e36-afbb-fd30443b3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurMushroom:\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = image_paths\n",
    "        self.class_to_index = class_to_index\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = ImageFile.open(img_path)\n",
    "        label = self.class_to_index[img_path.split(os.path.sep)[-2]]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694a0eb-9c20-4360-a4be-3fd840ce1264",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = OurMushroom(train_image_paths, transform = transform_train)\n",
    "test_data = OurMushroom(test_image_paths, transform = transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76296bae-bd13-43fa-8f71-f4278f12a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "for path in train_image_paths:\n",
    "    label_name = os.path.basename(os.path.dirname(path))\n",
    "    label_idx = class_to_index.get(label_name)\n",
    "    if label_idx is not None:\n",
    "        label_dict[label_idx] = label_dict.get(label_idx, 0) + 1\n",
    "\n",
    "sorted_label_dict = sorted(label_dict.items())\n",
    "print(sorted_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c04b08-1f4a-48af-b465-932d7766584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = [label_dict[i] for i in range(9)]\n",
    "total_num = sum(num)\n",
    "weights = [1 / count * total_num for count in num]\n",
    "max_weight = max(weights)\n",
    "final_weights = [weight / max_weight for weight in weights]\n",
    "print(final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6824a0f-7e08-457d-9866-677a67e6c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('Code is being executed using CUDA [GPU]\\n'f'Using GPU Device: {torch.cuda.get_device_name(device)}')\n",
    "else:\n",
    "    print('Code is being executed using CPU (slow)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c2821-166a-4f17-ac07-29b53ae8f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 9\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f83f9-6f55-4715-9973-1560d8390951",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet101(weights=None)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_class)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a8cf1-37f3-44e0-86b0-c0686868d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f28079-aec6-4964-bf3b-243b7ce33e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class, best_accuracy, number_of_epochs = 9, 0, 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9bca2-e832-41c1-a9f7-67e7e705c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(number_of_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "    \n",
    "        for images, labels in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            labels, images = labels.to(device), images.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct * 100.0 / len(train_data)\n",
    "    \n",
    "        model.eval()\n",
    "        test_loss, test_correct = 0.0, 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(test_loader):\n",
    "                labels, images = labels.to(device), images.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = test_correct * 100.0 / len(test_data)\n",
    "    \n",
    "        if test_accuracy >= best_accuracy:\n",
    "            print(f\"Detected better accuracy: {test_accuracy}. Replaced with {best_accuracy}\")\n",
    "            best_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), \"model.pth\")\n",
    "    \n",
    "        print(f\"Epoch: {epoch + 1} / {number_of_epochs}\\n\"\n",
    "              f\"Train Loss: {train_loss:.2f}, Train Accuracy: {train_accuracy:.2f}\\n\"\n",
    "              f\"Test Loss: {test_loss:.2f}, Test Accuracy: {test_accuracy:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325bffd-48d5-4d98-88bc-f7fa32137467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "model = models.resnet101(weights=None, num_classes=9)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "def predict_image(image_path, model):\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform_test(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        predicted_mushroom = torch.argmax(probabilities, dim=1).item()\n",
    "    return predicted_mushroom, probabilities[0, predicted_mushroom].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6232c-eed3-41e1-a1c2-4695506bac31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
